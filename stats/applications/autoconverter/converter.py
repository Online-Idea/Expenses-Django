import io
import logging
import random
import re
from collections import Counter
import ftplib
from ftplib import FTP
from functools import reduce
from pathlib import Path
import os

import demoji
import requests
import datetime
import xml.etree.ElementTree as ET

from django.db.models import QuerySet
from openpyxl import Workbook
# import xlsxwriter
import pandas as pd
from pandas import DataFrame
from telebot.types import InputFile

from applications.ads.models import Ad
from applications.autoconverter.models import *
from applications.autoconverter.onllline_base import onllline_worker
from libs.services.utils import get_models_verbose_names
from stats.settings import env
from libs.services.email_sender import send_email
from libs.services.management.commands.bot import bot, break_message_to_parts


# –°–ø–∏—Å–æ–∫ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: POST http://151.248.118.19/Api/Configurations/GetList
# –°–ø–∏—Å–æ–∫ –ü–∞–ø–æ–∫ —Å —Ñ–æ—Ç–æ: POST http://151.248.118.19/Api/Stock/GetClients

# –ü—Ä–æ–≥–æ–Ω —à–∞–±–ª–æ–Ω–∞ (3 –∑–∞–ø—Ä–æ—Å–∞)
# POST http://151.248.118.19/Api/Stock/StartProcess
# POST http://151.248.118.19/Api/Stock/GetProcessStep
# POST http://151.248.118.19/Api/Log/GetByProcessId

def get_converter_tasks(task_ids: list = None) -> QuerySet:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞. –ï—Å–ª–∏ –µ—Å—Ç—å task_ids —Ç–æ –±–µ—Ä—ë—Ç —Ç–æ–ª—å–∫–æ –∏—Ö, –∏–Ω–∞—á–µ –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ
    :param task_ids: —Å–ø–∏—Å–æ–∫ id –º–æ–¥–µ–ª–∏ ConverterTask
    :return: Django QuerySet —Å –ó–∞–¥–∞—á–∞–º–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    """
    if task_ids:
        return ConverterTask.objects.filter(id__in=task_ids)
    else:
        return ConverterTask.objects.filter(active=True)


def get_price(task):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞.
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    """
    template = converter_template(task)
    if template.empty:
        logging.info(f'–ü–æ –∫–ª–∏–µ–Ω—Ç—É {task.client.name} –ø—É—Å—Ç–æ–π —à–∞–±–ª–æ–Ω')
        return
    process_id = converter_post(task)
    price = converter_process_result(process_id, template, task)
    logs = converter_logs(process_id)
    logs_xlsx = logs_to_xlsx(logs, template, task)
    import_result = onllline_worker(task)
    message = f'–õ–æ–≥–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞:\n{logs}'
    if import_result:
        message += f'\n\n–û—Ç—á–µ—Ç –∏–º–ø–æ—Ä—Ç–∞ –±–∞–∑—ã:\n{import_result}'
    bot_messages(message, logs_xlsx, price, task)
    with open(logs_xlsx, 'rb') as file:
        file_content = file.read()
        save_on_ftp(logs_xlsx, file_content)
    os.remove(logs_xlsx)
    print(f'–ö–ª–∏–µ–Ω—Ç {task.slug} - –ø—Ä–∞–π—Å –≥–æ—Ç–æ–≤')
    return


def get_price_without_converter(task):
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –Ω–æ –±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞. –í –∫–∞—á–µ—Å—Ç–≤–µ —Å—Ç–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞—à –ø—Ä–∞–π—Å
    :param task:
    :return:
    """
    template = converter_template(task)
    if template.empty:
        logging.info(f'–ü–æ –∫–ª–∏–µ–Ω—Ç—É {task.client.name} –ø—É—Å—Ç–æ–π —à–∞–±–ª–æ–Ω')
        return
    price = converter_process_result(None, template, task)
    import_result = onllline_worker(task)
    message = ''
    if import_result:
        message += f'\n\n–û—Ç—á–µ—Ç –∏–º–ø–æ—Ä—Ç–∞ –±–∞–∑—ã:\n{import_result}'
    bot_messages(message, None, price, task)
    print(f'–ö–ª–∏–µ–Ω—Ç {task.slug} - –ø—Ä–∞–π—Å –≥–æ—Ç–æ–≤')


def converter_template(task):
    """
    –ò–∑ —Å—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ –¥–µ–ª–∞–µ—Ç —à–∞–±–ª–æ–Ω –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: —à–∞–±–ª–æ–Ω –∫–∞–∫ pandas dataframe
    """
    # –°–æ—Ö—Ä–∞–Ω—è—é —Å—Ç–æ–∫ –∫–ª–∏–µ–Ω—Ç–∞, –¥–µ–ª–∞—é –ø–æ –Ω–µ–º—É —à–∞–±–ª–æ–Ω –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    slug = task.slug
    file_date = str(datetime.datetime.now()).replace(' ', '_').replace(':', '-')
    stock_path = f'converter/{slug}/stocks/stock_{slug}_{file_date}'

    # –ü–æ–ª—É—á–∞—é —Å—Ç–æ–∫
    if task.stock_source == '–°—Å—ã–ª–∫–∞':
        response = requests.get(url=task.stock_url)
    elif task.stock_source == 'POST-–∑–∞–ø—Ä–æ—Å':
        data = {
            'login': task.stock_post_login,
            'password': task.stock_post_password,
        }
        response = requests.post(url=task.stock_post_host, data=data)
    else:
        raise ValueError('–ò—Å—Ç–æ—á–Ω–∏–∫ —Å—Ç–æ–∫–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å "–°—Å—ã–ª–∫–∞" –∏–ª–∏ "POST-–∑–∞–ø—Ä–æ—Å"')

    # –ü–æ–ª—É—á–∞—é —Ç–∏–ø —Ñ–∞–π–ª–∞ —Å—Ç–æ–∫–∞
    content_type = response.headers['content-type']
    xml_types = ['text/xml', 'application/xml']
    if any(ele in content_type for ele in xml_types):
        stock_path += '.xml'
        content_type = 'xml'
    elif 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' in content_type:
        stock_path += '.xlsx'
        content_type = 'xlsx'
    elif 'text/csv' in content_type:
        stock_path += '.csv'
        content_type = 'csv'

    # –°–æ—Ö—Ä–∞–Ω—è—é —Å—Ç–æ–∫ –Ω–∞ ftp
    os.makedirs(os.path.dirname(stock_path), mode=0o755, exist_ok=True)
    with open(stock_path, mode='wb') as file:
        file.write(response.content)
    # with open(stock_path, mode='w', encoding=task.stock_fields.encoding) as file:
    #     file.write(response.text)
    save_on_ftp(stock_path, response.content)

    # –ü—É—Ç—å —à–∞–±–ª–æ–Ω–∞
    template_path = f'converter/{slug}/templates/template_{slug}_{file_date}.xlsx'
    os.makedirs(os.path.dirname(template_path), exist_ok=True)
    task.template = template_path
    task.save()

    # –†–∞–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞ —Å—Ç–æ–∫–∞
    if content_type == 'xml':
        template = template_xml(stock_path, template_path, task)
    elif content_type == 'xlsx':
        template = template_xlsx_or_csv(stock_path, 'xlsx', template_path, task)
    elif content_type == 'csv':
        template = template_xlsx_or_csv(stock_path, 'csv', template_path, task)
    else:
        return '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞, –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å xml –∏–ª–∏ xlsx'

    # –£–±–∏—Ä–∞—é –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    template = template.map(lambda x: x.strip() if isinstance(x, str) else x)
    # template = template.applymap(lambda x: x.replace(' ', '') if isinstance(x, str) else x)

    with open(template_path, mode='rb') as file:
        file_content = file.read()
        save_on_ftp(template_path, file_content)
    os.remove(stock_path)

    return template


def template_xml(stock_path, template_path, task):
    """
    –®–∞–±–ª–æ–Ω –∏–∑ xml —Å—Ç–æ–∫–∞
    :param stock_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å—Ç–æ–∫–∞
    :param template_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: —à–∞–±–ª–æ–Ω –∫–∞–∫ pandas dataframe
    """
    # XML tree
    tree = ET.parse(stock_path)
    root = tree.getroot()

    # XLSX —à–∞–±–ª–æ–Ω
    workbook = Workbook()
    workbook.active.title = '–®–∞–±–ª–æ–Ω'
    sheet = workbook.active

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —à–∞–±–ª–æ–Ω–∞
    template_col = StockFields.TEMPLATE_COL
    for k, col in template_col.items():
        sheet.cell(row=1, column=col[1] + 1, value=col[0])

    # –î–∞–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω–∞
    fields = task.stock_fields
    exception_col = ['modification_code', 'options_code', 'images', 'modification_explained', 'description', 'vin']
    for i, car in enumerate(root.iter(fields.car_tag)):
        if stock_xml_filter(car, task):
            # –û–±—ã—á–Ω—ã–µ –ø–æ–ª—è
            for field in fields._meta.fields:
                field_val = getattr(fields, field.name)
                # –ï—Å–ª–∏ –Ω–µ –ø—É—Å—Ç–æ –ò –ø–æ–ª–µ –≤ –ø–æ–ª—è—Ö —à–∞–±–ª–æ–Ω–∞ –ò –ø–æ–ª–µ –ù–ï –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏—è—Ö
                if field_val and field.name in template_col and field.name not in exception_col:
                    cell = car.findtext(field_val)
                    try:
                        if cell.isnumeric():
                            cell = int(cell)
                    except AttributeError:
                        pass
                    sheet.cell(row=i + 2, column=template_col[field.name][1] + 1, value=cell)

            # –ü–æ–ª—è-–∏—Å–∫–ª—é—á–µ–Ω–∏—è
            if ',' in fields.modification_code:  # –ö–æ–¥ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                # –†–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ –∑–∞–ø—è—Ç–æ–π –≤ —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ. –£–±–∏—Ä–∞–µ—Ç –∑–∞–ø—è—Ç—É—é –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–∫–∞
                mod = [car.findtext(f).replace(',', '') for f in fields.modification_code.split(', ') if
                       car.findtext(f)]
                sheet.cell(row=i + 2, column=template_col['modification_code'][1] + 1, value=' | '.join(mod))
            else:
                sheet.cell(row=i + 2, column=template_col['modification_code'][1] + 1,
                           value=car.findtext(fields.modification_code))

            if fields.options_code:
                options = multi_tags(fields.options_code, car, ' ')  # –û–ø—Ü–∏–∏
                sheet.cell(row=i + 2, column=template_col['options_code'][1] + 1, value=options)

            if fields.images:
                if '–∞–≤–∏–ª–æ–Ω' in task.client.name.lower():
                    # –û—Å–æ–±–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ –æ—Ç –ê–≤–∏–ª–æ–Ω–∞
                    images = avilon_photos(fields.images, car)
                else:
                    images = multi_tags(fields.images, car, ' ')  # –§–æ—Ç–æ –∫–ª–∏–µ–Ω—Ç–∞
                sheet.cell(row=i + 2, column=template_col['images'][1] + 1, value=images)

            if ',' in fields.modification_explained:  # –†–∞—Å—à. –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                mod = [car.findtext(f) for f in fields.modification_explained.split(', ') if car.findtext(f)]
                sheet.cell(row=i + 2, column=template_col['modification_explained'][1] + 1, value=' | '.join(mod))
            else:
                sheet.cell(row=i + 2, column=template_col['modification_explained'][1] + 1,
                           value=car.findtext(fields.modification_explained))

            if fields.description:
                if ',' in fields.description:
                    descr = [car.findtext(f) for f in fields.description.split(', ') if car.findtext(f)]
                    descr = [f.replace("\\n", "") for f in descr]
                    sheet.cell(row=i + 2, column=template_col['description'][1] + 1, value=' \n\n '.join(descr))
                else:
                    sheet.cell(row=i + 2, column=template_col['description'][1] + 1,
                               # value=car.findtext(fields.description))
                               value=multi_tags(fields.description, car, '\n'))

            # VIN, –µ—Å–ª–∏ –º–µ–Ω—å—à–µ 17 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–æ –¥–æ–±–∞–≤–ª—è—é –≤ –Ω–∞—á–∞–ª–æ —Å—Ç–æ–ª—å–∫–æ 'X' —Å–∫–æ–ª—å–∫–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–æ 17
            vin = car.findtext(fields.vin)
            vin = f"{'X' * (17 - len(vin))}{vin}" if len(vin) < 17 else vin
            sheet.cell(row=i + 2, column=template_col['vin'][1] + 1, value=vin)

            # –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ —Å—Ç–æ–∫—É. –î–æ–±–∞–≤–ª—è—é —Å—Ç–æ–ª–±–µ—Ü –∫ —à–∞–±–ª–æ–Ω—É
            # extras = ConverterExtraProcessing.objects.filter(converter_task=task, source='–°—Ç–æ–∫')
            task_has_source_stock = Conditional.objects.filter(converter_extra_processing__converter_task=task,
                                                               source='–°—Ç–æ–∫')
            if task_has_source_stock:
                # –ë–µ—Ä—É –û–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞ –∏ –ù–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–∞—Å–∫–∞
                extras = ConverterExtraProcessing.objects.filter(converter_task=task)
                extras_new_changes = ConverterExtraProcessingNewChanges.objects.filter(
                    converter_extra_processing__in=extras)
                for extra in extras:
                    conditionals = list(Conditional.objects.filter(converter_extra_processing=extra)
                                        .values('field'))
                    curr_extra_new_changes = extras_new_changes.filter(converter_extra_processing=extra)

                    for new_change in curr_extra_new_changes:
                        # –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—Ä–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –¥—Ä—É–≥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
                        if new_change.new_value[:4] == '%col':
                            column_name_extra = re.findall(r'"(.*?)"', new_change.new_value)[0]
                            column_name_extra += '__stock'
                            conditionals.append({'field': column_name_extra})

                    for cond in conditionals:
                        column_name = cond['field']
                        if '__stock' in column_name:
                            value = multi_tags(column_name.replace('__stock', ''), car, ' ')
                        else:
                            value = multi_tags(column_name, car, ' ')

                        if column_name not in template_col:
                            max_column = len(template_col)
                            template_col[column_name] = (column_name, max_column)
                            sheet.cell(row=1, column=max_column + 1, value=column_name)

                        column_number = template_col[column_name][1] + 1

                        sheet.cell(row=i + 2, column=column_number, value=value)

    # –£–¥–∞–ª—è—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    for row in reversed(range(1, sheet.max_row + 1)):
        if all(cell.value is None for cell in sheet[row]):
            sheet.delete_rows(row)

    # –£–¥–∞–ª—è—é –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    for row in workbook.active.iter_rows():
        for cell in row:
            if isinstance(cell.value, str):
                cell.value = cell.value.rstrip()

    workbook.save(filename=template_path)

    # –ß—Ç–æ–±—ã pandas –Ω–µ –º–µ–Ω—è–ª –∫–æ–ª–æ–Ω–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º true –∏–ª–∏ false –Ω–∞ bool, –æ—Å—Ç–∞–≤–ª—è–ª –∫–∞–∫ str
    # - —Ç–æ–≥–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞ (ConverterExtraProcessing) –±—É–¥—É—Ç –≤–µ—Ä–Ω–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
    template_df = pd.read_excel(template_path, decimal=',')
    bool_cols_as_str = {}
    for col in template_df.columns:
        unique_values = set(x.strip().lower() for x in template_df[col].dropna().unique().astype(str))
        if unique_values.issubset({'true', 'false'}):
            bool_cols_as_str[col] = 'str'

    template_df = pd.read_excel(template_path, decimal=',', dtype=bool_cols_as_str)

    return template_df


def avilon_photos(field: str, element: ET.Element) -> str:
    """
    –û—Å–æ–±–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –ê–≤–∏–ª–æ–Ω–∞. –ï—Å–ª–∏ –±—Ä–∞—Ç—å –∏—Ö —Ñ–æ—Ç–æ –≤ —Ç–æ–º –ø–æ—Ä—è–¥–∫–µ –∫–∞–∫ –æ–Ω–∏ –∏–¥—É—Ç –≤ xml,
    —Ç–æ –ø–æ—Ä—è–¥–æ–∫ —Ñ–æ—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–º. –ê —Ç–∞–∫ –∫–∞–∫ –ê–≤–∏–ª–æ–Ω –∫ –∫–∞–∂–¥–æ–º—É —Ñ–æ—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç —É–≥–æ–ª,
    —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω–∏ —Å–Ω–∏–º–∞–ª–∏, —Ç–æ –º–æ–∂–Ω–æ –ø–æ —ç—Ç–æ–º—É —É–≥–ª—É –≤—ã—Å—Ç–∞–≤–ª—è—Ç—å –≤ –∫—Ä–∞—Å–∏–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ —Ñ–æ—Ç–æ
    :param field: –∏–º—è —Ç—ç–≥–∞ —Å —Ñ–æ—Ç–æ
    :param element: —ç–ª–µ–º–µ–Ω—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—è
    :return: —Ñ–æ—Ç–æ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –∑–∞–ø—è—Ç–æ–π
    """

    def get_by_view_type(tags, view_type):
        return [tag for tag in tags if tag.attrib['view_type'] == view_type]

    tags = element.findall(field)

    exterior = get_by_view_type(tags, 'exterior')
    # –ü–µ—Ä–≤–æ–µ —Ñ–æ—Ç–æ –Ω–µ–º–Ω–æ–≥–æ –±–æ–∫–æ–º –≤–ª–µ–≤–æ
    for element in exterior:
        if element.attrib['angle'] in ['40', '320']:
            element.set('angle', '-40')
            break
    exterior.sort(key=lambda x: float(x.get('angle')))

    detail = get_by_view_type(tags, 'detail')

    interior = get_by_view_type(tags, 'interior')

    images = [image.text for image in exterior]
    images += [image.text for image in detail]
    images += [image.text for image in interior]
    return ', '.join(images)


def stock_xml_filter(car, task):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—å –∏–∑ xml —Å—Ç–æ–∫–∞ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º –∏–∑ ConverterFilter
    :param car: node –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ xml —Å—Ç–æ–∫–∞
    :param task: task (–∑–∞–ø–∏—Å—å) –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: True –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–æ–π–¥–µ–Ω—ã
    """
    filters = ConverterFilter.objects.filter(converter_task=task, active=True)
    dict_filters, stock_fields, result = [], [], []

    # –ü–µ—Ä–µ–≤–æ–∂—É —Ñ–∏–ª—å—Ç—Ä—ã –≤ —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞: {'values': values, 'condition': condition, 'field': field}
    for f in filters:
        # –ó–Ω–∞—á–µ–Ω–∏—è
        if '`' in f.value:
            values = [val.replace('`', '').strip() for val in f.value.split('`,')]
        else:
            values = [f.value]

        # –ü–æ–ª—è (—Ç–µ–≥–∏)
        if '/' in f.field:  # –ü—É—Ç—å –∫ –¥–µ—Ç—è–º
            parent, child = f.field.rsplit('/', 1)

            if '@' not in f.field:  # –î–µ—Ç–∏
                stock_fields = [tag.text for tag in car.find(parent) if tag.tag == child]

            else:  # –ê—Ç—Ä–∏–±—É—Ç—ã
                attribute_name = f.field.split('@')[1].split('=')[0]
                attribute_value = f.field.split('"')[1].replace('"', '')
                for tag in car.find(parent):
                    for tag_attribute in tag.attrib.items():
                        if tag_attribute == (attribute_name, attribute_value):
                            stock_fields = [tag.text]

        else:  # –ü—Ä–æ—Å—Ç–æ –æ–¥–∏–Ω —Ç–µ–≥
            stock_fields = [car.findtext(f.field)]

        for field in stock_fields:
            dict_filters.append({
                'values': values,
                'condition': f.condition,
                'field': field,
            })

    for sf in dict_filters:
        for value in sf['values']:
            result.append(xml_filter_conditions(value, sf['condition'], sf['field']))

    # –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ —Ä–∞–≤–Ω–∞ —Å—É–º–º–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–Ω–∞—á–∏—Ç –∫–∞–∂–¥—ã–π —Ñ–∏–ª—å—Ç—Ä –≤–µ—Ä–Ω—É–ª True, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—å –ø–æ–¥—Ö–æ–¥–∏—Ç
    return len(dict_filters) == sum(result)


def xml_filter_conditions(value, condition, stock_field):
    """
    –£—Å–ª–æ–≤–∏—è —Ñ–∏–ª—å—Ç—Ä–∞
    :param value: –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    :param condition: —É—Å–ª–æ–≤–∏–µ
    :param stock_field: –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å—Ç–æ–∫–∞
    :return: True –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ
    """
    if condition == ConverterFilter.GREATER_THAN:
        if not stock_field:
            stock_field = 0
        return eval(f'{int(stock_field)} {condition} {int(value)}')
    elif condition == ConverterFilter.LESS_THAN:
        if not stock_field:
            stock_field = 0
        return eval(f'{int(stock_field)} {condition} {int(value)}')
    elif 'with' not in condition:
        return eval(f'"{value}" {condition} "{stock_field}"')
    elif condition == ConverterFilter.STARTS_WITH:
        return stock_field.startswith(value)
    elif condition == ConverterFilter.NOT_STARTS_WITH:
        return not (stock_field.startswith(value))
    elif condition == ConverterFilter.ENDS_WITH:
        return stock_field.endswith(value)
    elif condition == ConverterFilter.NOT_ENDS_WITH:
        return not (stock_field.endswith(value))


def template_xlsx_or_csv(stock_path, filetype, template_path, task):
    """
    –®–∞–±–ª–æ–Ω –∏–∑ xlsx –∏–ª–∏ csv —Å—Ç–æ–∫–∞
    :param stock_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å—Ç–æ–∫–∞
    :param filetype: —Ç–∏–ø —Ñ–∞–π–ª–∞: 'xlsx' –∏–ª–∏ 'csv'
    :param template_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: —à–∞–±–ª–æ–Ω –∫–∞–∫ pandas dataframe
    """
    if filetype == 'xlsx':
        df_stock = pd.read_excel(stock_path, decimal=',')
    elif filetype == 'csv':
        encoding = task.stock_fields.encoding
        df_stock = pd.read_csv(stock_path, decimal=',', sep=';', header=0, encoding=encoding)
    else:
        return '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞, –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å xlsx –∏–ª–∏ csv'

    # –ß—Ç–æ–±—ã pandas –Ω–µ –º–µ–Ω—è–ª –∫–æ–ª–æ–Ω–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º true –∏–ª–∏ false –Ω–∞ bool, –æ—Å—Ç–∞–≤–ª—è–ª –∫–∞–∫ str
    # - —Ç–æ–≥–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞ (ConverterExtraProcessing) –±—É–¥—É—Ç –≤–µ—Ä–Ω–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
    bool_cols_as_str = {}
    for col in df_stock.columns:
        unique_values = set(x.strip().lower() for x in df_stock[col].dropna().unique().astype(str))
        if unique_values.issubset({'true', 'false'}):
            bool_cols_as_str[col] = 'str'

    if filetype == 'xlsx':
        df_stock = pd.read_excel(stock_path, decimal=',', dtype=bool_cols_as_str)
    elif filetype == 'csv':
        encoding = task.stock_fields.encoding
        df_stock = pd.read_csv(stock_path, decimal=',', sep=';', header=0, encoding=encoding, dtype=bool_cols_as_str)

    # –ü—Ä–æ–≤–µ—Ä—è—é –µ—Å–ª–∏ —Å—Ç–æ–∫ —ç—Ç–æ –Ω–∞—à –ø—Ä–∞–π—Å. –ù–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –ø—Ä–∞–π—Å –≥–æ—Ç–æ–≤.
    # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–µ 4 —Å—Ç–æ–ª–±—Ü–∞ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å our_price_first_4_cols –ò –ò—Å—Ö–æ–¥–Ω—ã–π VIN –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö
    our_price_first_4_cols = ['–ú–∞—Ä–∫–∞', '–ú–æ–¥–µ–ª—å', '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', '–ê–≤—Ç–æ.—Ä—É –ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è']
    if list(df_stock.columns[:4]) == our_price_first_4_cols and '–ò—Å—Ö–æ–¥–Ω—ã–π VIN' in df_stock.columns:
        # df_stock.loc[df_stock['–§–æ—Ç–æ'].str.contains('gallery'), '–§–æ—Ç–æ'] = ''
        df_stock.T.reset_index().T.to_excel(template_path, sheet_name='–®–∞–±–ª–æ–Ω', header=False, index=False)
        return df_stock

    df_stock = stock_xlsx_filter(df_stock, task)

    fields = StockFields.objects.filter(pk=task.stock_fields.id)
    fields = fields.values()[0]
    template_col = StockFields.TEMPLATE_COL

    # –î–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤ –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥—Ä—É–≥–∏—Ö
    for k, field in fields.copy().items():
        if ',' in str(field):
            columns_split = field.split(', ')
            df_stock[template_col[k][0]] = df_stock[columns_split[0]].str.cat(df_stock[columns_split[1:]]
                                                                              .astype(str), sep=' | ')
            del fields[k]

    # –î–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤ —É –∫–æ—Ç–æ—Ä—ã—Ö –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫.
    # –ù–∞–ø—Ä–∏–º–µ—Ä –∫–æ–≥–¥–∞ '–ö–æ–¥ —Ü–≤–µ—Ç–∞' –∏ '–†–∞—Å—à. —Ü–≤–µ—Ç–∞' —ç—Ç–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü –≤ —Å—Ç–æ–∫–µ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–¶–≤–µ—Ç'
    same_origin_columns_counter = Counter(fields.values())
    del same_origin_columns_counter[None]
    df_stock_copy = pd.DataFrame()
    # –î–æ–±–∞–≤–ª—è—é –∫–∞–∂–¥—ã–π —Ç–∞–∫–æ–π —Å—Ç–æ–ª–±–µ—Ü –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ dataframe
    for k, v in fields.copy().items():
        if same_origin_columns_counter[v] > 1:
            df_stock_copy[template_col[k][0]] = df_stock[v]
            del fields[k]
    # –ú–µ–Ω—è—é –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤
    swapped_cols = {v: template_col[k][0] for k, v in fields.items() if k in template_col}
    # –û–±—ä–µ–¥–∏–Ω—è—é
    combined_dfs = [df_stock_copy, df_stock.rename(columns=swapped_cols)]
    df_stock_copy = pd.concat(combined_dfs, axis=1)

    # –î–æ–±–∞–≤–ª—è—é –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
    cur_cols = list(df_stock_copy.columns.values)
    for k, col in template_col.items():
        if col[0] not in cur_cols:
            df_stock_copy[col[0]] = ''

    # –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ —Å—Ç–æ–∫—É. –î–æ–±–∞–≤–ª—è—é —Å—Ç–æ–ª–±–µ—Ü –∫ —à–∞–±–ª–æ–Ω—É
    # extras = ConverterExtraProcessing.objects.filter(converter_task=task, source='–°—Ç–æ–∫')
    task_has_source_stock = Conditional.objects.filter(converter_extra_processing__converter_task=task, source='–°—Ç–æ–∫')
    if task_has_source_stock:
        # –ë–µ—Ä—É –û–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞ –∏ –ù–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–∞—Å–∫–∞
        extras = ConverterExtraProcessing.objects.filter(converter_extra_processing__converter_task=task)
        for extra in extras:
            conditionals = list(Conditional.objects.filter(converter_extra_processing=extra).values('field'))
            for cond in conditionals:
                column_name = cond.field
                if column_name not in template_col:
                    max_column = len(template_col)
                    template_col[column_name] = (column_name, max_column)

    # –û—Å—Ç–∞–≤–ª—è—é —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –≤ —Ç–æ–º –ø–æ—Ä—è–¥–∫–µ –∫–∞–∫ –≤ template_col
    df_stock_copy = df_stock_copy[[v[0] for k, v in template_col.items()]]
    # –í –û–ø—Ü–∏–∏ –∏ –ø–∞–∫–µ—Ç—ã –∑–∞–º–µ–Ω—è—é –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –ø—Ä–æ–±–µ–ª
    df_stock_copy['–û–ø—Ü–∏–∏ –∏ –ø–∞–∫–µ—Ç—ã'].replace(r'\n', ' ', regex=True, inplace=True)
    # –£–±–∏—Ä–∞—é –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    df_stock_copy = df_stock_copy.applymap(lambda x: x.strip() if isinstance(x, str) else x)

    df_stock_copy.T.reset_index().T.to_excel(template_path, sheet_name='–®–∞–±–ª–æ–Ω', header=False, index=False)

    return df_stock_copy


def stock_xlsx_filter(df, task):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª—å –∏–∑ xlsx —Å—Ç–æ–∫–∞ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º –∏–∑ ConverterFilter
    :param df: xlsx —Å—Ç–æ–∫ –≤ –≤–∏–¥–µ pandas dataframe
    :param task: task (–∑–∞–ø–∏—Å—å) –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π dataframe
    """
    filters = ConverterFilter.objects.filter(converter_task=task, active=True)
    filter_strings = []
    for f in filters:
        filter_or = []
        if '`' in f.value:
            values = [val.replace('`', '').strip() for val in f.value.split('`,')]
        else:
            values = [f.value]

        for value in values:
            if f.condition == ConverterFilter.CONTAINS:
                filter_or.append(f'(df["{f.field}"].str.contains({value})')
            elif f.condition == ConverterFilter.NOT_CONTAINS:
                filter_or.append(f'(~df["{f.field}"].str.contains({value})')
            elif f.condition == ConverterFilter.EQUALS:
                filter_or.append(f'(df["{f.field}"] == "{value}")')
            elif f.condition == ConverterFilter.NOT_EQUALS:
                filter_or.append(f'(~df["{f.field}"] == "{value}")')
            elif f.condition == ConverterFilter.GREATER_THAN:
                filter_or.append(f'(df["{f.field}"] > "{value}")')
            elif f.condition == ConverterFilter.LESS_THAN:
                filter_or.append(f'(df["{f.field}"] < "{value}")')
            elif f.condition == ConverterFilter.STARTS_WITH:
                filter_or.append(f'(df["{f.field}"].str.startswith("{value}"))')
            elif f.condition == ConverterFilter.NOT_STARTS_WITH:
                filter_or.append(f'~(df["{f.field}"].str.startswith("{value}"))')
            elif f.condition == ConverterFilter.ENDS_WITH:
                filter_or.append(f'(df["{f.field}"].str.endswith("{value}"))')
            elif f.condition == ConverterFilter.NOT_ENDS_WITH:
                filter_or.append(f'~(df["{f.field}"].str.endswith("{value}"))')

        filter_strings.append(f'({" | ".join(filter_or)})')

    return eval(f'df.loc[{" & ".join(filter_strings)}]') if filter_strings else df


def price_extra_processing(df: DataFrame, task: ConverterTask, template: DataFrame) -> DataFrame:
    """
    –ú–µ–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–π—Å–µ –ø–æ —É—Å–ª–æ–≤–∏—é
    :param df: –ø—Ä–∞–π—Å –≤ –≤–∏–¥–µ pandas dataframe
    :param task: task (–∑–∞–ø–∏—Å—å) –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :param template: —à–∞–±–ª–æ–Ω –≤ –≤–∏–¥–µ pandas dataframe
    :return: –ì–æ—Ç–æ–≤—ã–π –ø—Ä–∞–π—Å —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
    """
    extra_processings = ConverterExtraProcessing.objects.filter(converter_task=task)
    conditionals = Conditional.objects.filter(converter_extra_processing__in=extra_processings)
    new_changes = ConverterExtraProcessingNewChanges.objects.filter(converter_extra_processing__in=extra_processings)

    price_columns = get_models_verbose_names(Ad)
    price_columns.append('–§–æ—Ç–æ')

    # –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Å—Ç–æ–∫–µ —Ç–æ —Å–Ω–∞—á–∞–ª–∞ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫ —à–∞–±–ª–æ–Ω—É.
    # –ü–æ—Ç–æ–º —à–∞–±–ª–æ–Ω –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —Å—é–¥–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –ø—Ä–∞–π—Å—É.
    if conditionals.filter(source='–°—Ç–æ–∫'):
        template = template.add_suffix('_template')
        df = pd.merge(df, template, left_index=True, right_index=True)

    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ –∫–ª–∏–µ–Ω—Ç—É
    for change in new_changes:
        curr_conditionals = conditionals.filter(converter_extra_processing=change.converter_extra_processing)
        masks = []

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Å–ª–æ–≤–∏—è –≤ –∏–∑–º–µ–Ω–µ–Ω–∏–∏
        for cond in curr_conditionals:
            if cond.source == '–°—Ç–æ–∫':
                cond.field += '_template'

            # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π
            if '`' in cond.value:
                values = [val.replace('`', '').strip() for val in cond.value.split('`,')]
            else:
                values = [cond.value]

            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            or_masks = []
            for value in values:
                if value.isdigit():
                    value = int(value)

                if cond.condition == ConverterFilter.CONTAINS:
                    or_masks.append(df[cond.field].str.contains(value))
                elif cond.condition == ConverterFilter.NOT_CONTAINS:
                    or_masks.append(~df[cond.field].str.contains(value))
                elif cond.condition == ConverterFilter.EQUALS:
                    or_masks.append(df[cond.field] == value)
                elif cond.condition == ConverterFilter.NOT_EQUALS:
                    or_masks.append(df[cond.field] != value)
                elif cond.condition == ConverterFilter.GREATER_THAN:
                    or_masks.append(df[cond.field] > value)
                elif cond.condition == ConverterFilter.LESS_THAN:
                    or_masks.append(df[cond.field] < value)
                elif cond.condition == ConverterFilter.STARTS_WITH:
                    or_masks.append(df[cond.field].str.startswith(value))
                elif cond.condition == ConverterFilter.NOT_STARTS_WITH:
                    or_masks.append(~df[cond.field].str.startswith(value))
                elif cond.condition == ConverterFilter.ENDS_WITH:
                    or_masks.append(df[cond.field].str.endswith(value))
                elif cond.condition == ConverterFilter.NOT_ENDS_WITH:
                    or_masks.append(~df[cond.field].str.endswith(value))

            # –ù–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ò–õ–ò
            combined_or_mask = reduce(lambda x, y: x | y, or_masks)
            combined_or_mask = combined_or_mask.fillna(False)
            masks.append(combined_or_mask)

        # –û–±—ä–µ–¥–∏–Ω—è—é –º–∞—Å–∫–∏
        combined_mask = reduce(lambda x, y: x & y, masks)

        # –ü—Ä–æ—Å—Ç–∞–≤–ª—è—é –Ω–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        # –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—Ä–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –¥—Ä—É–≥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
        if change.new_value and change.new_value[:4] == '%col':
            column = re.findall(r'"(.*?)"', change.new_value)[0]
            # if change.source == '–°—Ç–æ–∫':
            #     column += '__stock_template'
            if column not in price_columns:
                column += '__stock_template'
            df.loc[combined_mask, change.price_column_to_change] = df[column]
        # –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –Ω–∞—á–∞–ª–æ
        elif change.change_type == '–î–æ–±–∞–≤–∏—Ç—å –≤ –Ω–∞—á–∞–ª–æ':
            df.loc[combined_mask, change.price_column_to_change] = df.loc[combined_mask, change.price_column_to_change] \
                .apply(lambda x: change.new_value + str(x))
        # –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü
        elif change.change_type == '–î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü':
            df.loc[combined_mask, change.price_column_to_change] = df.loc[combined_mask, change.price_column_to_change] \
                .apply(lambda x: str(x) + change.new_value)
        # –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é
        elif change.change_type == '–ü–æ–ª–Ω–æ—Å—Ç—å—é':
            df.loc[combined_mask, change.price_column_to_change] = change.new_value

    df = df.drop(df.filter(regex='_template').columns, axis=1)

    return df


def multi_tags(field, element, delimiter):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª—è –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –¥–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–≥–æ–≤
    :param field: –ø–æ–ª–µ
    :param element: —ç–ª–µ–º–µ–Ω—Ç –∏–∑ xml
    :param delimiter: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    :return: –≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —à–∞–±–ª–æ–Ω–∞
    """
    result = []

    if '/' in field:
        parent = field.split('/')[0]
        parent_element = element.find(parent)
        if parent_element:
            if '@' not in field:  # –ï—Å–ª–∏ —Ç–µ–≥ —Å –¥–µ—Ç—å–º–∏ –∏ –Ω—É–∂–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–µ—Ç–µ–π
                result = [tag.text for tag in parent_element]
            else:  # –ï—Å–ª–∏ —Ç–µ–≥ —Å –¥–µ—Ç—å–º–∏ –∏ –∏–∑ –¥–µ—Ç–µ–π –Ω—É–∂–µ–Ω –∞—Ç—Ä–∏–±—É—Ç
                attribute = field.split('@')[1]
                result = [tag.attrib[attribute] for tag in parent_element]
    else:
        if '@' not in field:  # –ï—Å–ª–∏ —Ç–µ–≥ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –∏ –Ω—É–∂–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
            tags = element.findall(field)
            if tags:
                result = [tag.text for tag in tags]
        else:  # –ï—Å–ª–∏ —Ç–µ–≥ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –∏ –∏–∑ –Ω–µ–≥–æ –Ω—É–∂–µ–Ω –∞—Ç—Ä–∏–±—É—Ç
            tag_name, attribute = field.split('@')
            tags = element.findall(tag_name)
            if tags:
                result = [tag.attrib[attribute] for tag in tags]

    return delimiter.join(result)


def converter_post(task):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —à–∞–±–ª–æ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ –∫–æ—Ç–æ—Ä—ã–º –Ω—É–∂–Ω–æ –ø—Ä–æ–≥–Ω–∞—Ç—å –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä
    :param task: task (–∑–∞–ø–∏—Å—å) –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: processId –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –¥–∞–ª—å—à–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–π—Å
    """
    # –û—Ç–ø—Ä–∞–≤–ª—è—é —à–∞–±–ª–æ–Ω
    url = 'http://151.248.118.19/Api/Stock/StartProcess'

    configuration = task.configuration.configuration if task.configuration is not None else Configuration.DEFAULT
    payload = {
        'client': task.photos_folder.folder,
        'configuration': configuration,
        'frontLength': task.front,
        'backLength': task.back,
        'interiorsLength': task.interior,
        'onlySalon': task.salon_only,
    }
    template = open(task.template, 'rb')
    files = {'file': ('template.xlsx', template, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                      {'Expires': '0'})}
    response = requests.post(url=url, data=payload, files=files)
    template.close()
    os.remove(task.template)
    return response.json()['processId']


def converter_get_price_by_process_id(task, process_id):
    """
    # –ü–æ–ª—É—á–∞—é –ø—Ä–∞–π—Å –æ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ –ø–æ process_id
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :param process_id: –∏–∑ converter_post
    :return: –ø—Ä–∞–π—Å –∫–∞–∫ pandas dataframe
    """
    client = task.slug
    url = 'http://151.248.118.19/Api/Stock/GetProcessResult'
    payload = {'processId': (None, process_id)}
    response = requests.post(url=url, files=payload)

    # –°–æ—Ö—Ä–∞–Ω—è—é –ø—Ä–∞–π—Å –≤ xlsx
    file_date = str(datetime.datetime.now()).replace(' ', '_').replace(':', '-')
    save_path_date = f'converter/{client}/prices/price_{client}_{file_date}.xlsx'
    os.makedirs(os.path.dirname(save_path_date), exist_ok=True)
    with open(save_path_date, 'wb') as file:
        file.write(response.content)
    save_on_ftp(save_path_date, response.content)

    # –û–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∞–π—Å–∞
    read_file = pd.read_excel(save_path_date, decimal=',')

    # –î–æ–±–∞–≤–ª—è—é —Å—Ç–æ–ª–±—Ü—ã –±–∞–∑—ã –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø—Ä–∞–π—Å–µ –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    read_file.insert(37, '–î–∞—Ç–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è', '')
    read_file.insert(38, '–ö–æ–ª. –≤ –ü–¢–°', '')
    end_columns = ['ComID', '–û–Ω–ª–∞–π–Ω-–ø–æ–∫–∞–∑', 'Avito gen', 'Avito mod', 'Avito com', 'Avito mid',
                   '–ì–∞—Ä–∞–Ω—Ç–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è', '–í–∞–ª—é—Ç–∞', '–ê–≤–∏—Ç–æ –∞—É–∫—Ü–∏–æ–Ω', '–î–≤–µ—Ä–∏', '–ì–∞—Ä–∞–Ω—Ç–∏—è –Ω–∞ —Ä–µ–º–æ–Ω—Ç',
                   '–ö–ê–°–ö–û –≤ –ø–æ–¥–∞—Ä–æ–∫', '–ö–æ–º–ø–ª–µ–∫—Ç —à–∏–Ω –≤ –ø–æ–¥–∞—Ä–æ–∫', '–°–∫–∏–¥–∫–∞ –Ω–∞ –ö–ê–°–ö–û', '–¢–û –≤ –ø–æ–¥–∞—Ä–æ–∫',
                   '–ê–∫—Å–µ—Å—Å—É–∞—Ä—ã –≤ –ø–æ–¥–∞—Ä–æ–∫', '–ë—ã—Å—Ç—Ä–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ']
    for col in end_columns:
        read_file[col] = ''

    os.remove(save_path_date)
    return read_file


def converter_process_result(process_id, template, task):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π –ø—Ä–∞–π—Å
    :param process_id: –∏–∑ converter_post
    :param template: —à–∞–±–ª–æ–Ω –∫–∞–∫ pandas dataframe - –µ—Å–ª–∏ –∏–∑ —à–∞–±–ª–æ–Ω–∞ –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–∞–π—Å–∞
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    """
    client = task.slug
    if task.use_converter:
        read_file = converter_get_price_by_process_id(task, process_id)
    else:
        read_file = template

    # –ü–µ—Ä–µ–Ω–æ—à—É –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ —à–∞–±–ª–æ–Ω–∞ –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å –≤ —à–∞–±–ª–æ–Ω–µ
    if '–û–ø–∏—Å–∞–Ω–∏–µ' in template.columns:
        read_file['–û–ø–∏—Å–∞–Ω–∏–µ2'] = template['–û–ø–∏—Å–∞–Ω–∏–µ']
        read_file.loc[read_file['–û–ø–∏—Å–∞–Ω–∏–µ2'].notnull(), '–û–ø–∏—Å–∞–Ω–∏–µ'] = read_file['–û–ø–∏—Å–∞–Ω–∏–µ2']
        read_file.drop(columns='–û–ø–∏—Å–∞–Ω–∏–µ2', inplace=True)

    # –£–±–∏—Ä–∞—é –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∞–Ω—ã (–ø—É—Å—Ç—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ú–∞—Ä–∫–∞, –¶–≤–µ—Ç –ª–∏–±–æ –§–æ—Ç–æ)
    read_file = read_file[(~read_file['–ú–∞—Ä–∫–∞'].isnull()) &
                          (~read_file['–¶–≤–µ—Ç'].isnull()) &
                          (~read_file['–§–æ—Ç–æ'].isnull())]
    # –ï—Å–ª–∏ –ú–∞–∫—Å–∏–º–∞–ª–∫–∞ –ø—É—Å—Ç–∞—è —Ç–æ —Å—á–∏—Ç–∞—Ç—å –µ—ë –∫–∞–∫ —Å—É–º–º—É –¥—Ä—É–≥–∏—Ö —Å–∫–∏–¥–æ–∫
    read_file.loc[read_file['–ú–∞–∫—Å–∏–º–∞–ª–∫–∞'].isna(), '–ú–∞–∫—Å–∏–º–∞–ª–∫–∞'] = read_file[['–¢—Ä–µ–π–¥-–∏–Ω', '–ö—Ä–µ–¥–∏—Ç', '–°—Ç—Ä–∞—Ö–æ–≤–∫–∞']].sum(
        axis=1)

    # –†–∞–∑–ª–∏—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–π—Å–∞ –ø–æ —É—Å–ª–æ–≤–∏—é
    read_file = price_extra_processing(read_file, task, template)

    # –ú–µ–ª–∫–∏–µ –∑–∞–º–µ–Ω—ã
    read_file.fillna('', inplace=True)
    read_file = read_file.astype(str).replace(
        {
            r"\.0$": "",
            "√©": "e",
            "\u2070": "0",
            "\xb3": "",
            "\uff08": "",
            "\uff09": "",
        },
        regex=True,
    )
    read_file = read_file.map(lambda x: demoji.replace(x, ''))
    read_file['–û–ø–∏—Å–∞–Ω–∏–µ'] = read_file['–û–ø–∏—Å–∞–Ω–∏–µ'].replace('_x000d_', '', regex=True)

    # –î–æ–±–∞–≤–ª—è—é –æ–±—ä—è–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Å—Ç–æ–∫–µ –∫–ª–∏–µ–Ω—Ç–∞, —Å –¥—Ä—É–≥–æ–≥–æ —Ñ–∞–π–ª–∞
    if task.add_to_price:
        if task.add_to_price.endswith('.csv'):
            add_manually_df = pd.read_csv(task.add_to_price, decimal=',', sep=';', header=0, encoding='cp1251')
            float_columns = ['–¶–µ–Ω–∞', '–¢—Ä–µ–π–¥-–∏–Ω', '–ö—Ä–µ–¥–∏—Ç', '–°—Ç—Ä–∞—Ö–æ–≤–∫–∞', '–ú–∞–∫—Å–∏–º–∞–ª–∫–∞']
            add_manually_df[float_columns] = add_manually_df[float_columns].fillna(0).astype(int)
        elif task.add_to_price.endswith('.xlsx'):
            add_manually_df = pd.read_excel(task.add_to_price, decimal=',')
        else:
            raise ValueError('–§–∞–π–ª —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å csv –∏–ª–∏ xlsx')
        read_file = pd.concat([read_file, add_manually_df], axis=0)

    if task.change_vin:
        read_file = change_vin_to_random(read_file)

    # –°–æ—Ö—Ä–∞–Ω—è—é –≤ csv
    save_path = f'converter/{client}/prices/price_{client}.csv'

    # string_buffer = io.StringIO()
    # read_file.to_csv(string_buffer, sep=';', header=True, index=False, decimal=',')
    # csv_string = string_buffer.getvalue()
    #
    # with open(save_path, 'w', encoding='cp1251', errors='ignore') as f:
    #     f.write(csv_string)
    read_file.to_csv(save_path, sep=';', header=True, index=False, decimal=',', encoding='cp1251', errors='ignore',
                     lineterminator='\n')
    with open(save_path, 'rb') as file:
        file_content = file.read()
        save_on_ftp(save_path, file_content)

    task.price = save_path
    task.save()

    os.remove(save_path)
    return read_file


def converter_logs(process_id):
    """
    –õ–æ–≥–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: –ª–æ–≥–∏ –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞
    """
    # –õ–æ–≥–∏ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä
    url = 'http://151.248.118.19/Api/Log/GetByProcessId'
    payload = {'processId': process_id}
    response = requests.post(url=url, json=payload)
    logs = response.json()['log']
    logs = logs.replace(' ,', '')  # –£–±–∏—Ä–∞—é –ª–∏—à–Ω–∏–µ –∑–∞–ø—è—Ç—ã–µ
    return logs


def logs_to_xlsx(logs, template, task):
    """
    –õ–æ–≥–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ –≤–º–µ—Å—Ç–µ —Å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–æ–π –≤ xlsx
    :param logs: –ª–æ–≥–∏ –æ—Ç converter_logs
    :param template: —à–∞–±–ª–æ–Ω –æ—Ç converter_template
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    :return: xlsx —Ñ–∞–π–ª –ª–æ–≥–æ–≤
    """
    lookup_cols = {
        # –ë–∞–∑–∞ –∏–∑ –ª–æ–≥–∞: (–ò–º—è —Å—Ç–æ–ª–±—Ü–∞ —Å –∫–æ–¥–æ–º, –ò–º—è —Å—Ç–æ–ª–±—Ü–∞ —Å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–æ–π)
        '–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏': ('–ö–æ–¥ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏', '–†–∞—Å—à. –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏'),
        '–¶–≤–µ—Ç–∞': ('–ö–æ–¥ —Ü–≤–µ—Ç–∞', '–†–∞—Å—à. —Ü–≤–µ—Ç–∞'),
        '–ò–Ω—Ç–µ—Ä—å–µ—Ä—ã': ('–ö–æ–¥ –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞', '–†–∞—Å—à. –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞'),
        '–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏–∏': ('–ö–æ–¥ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏', '–†–∞—Å—à. –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏'),
        # '–û–ø—Ü–∏–∏': –≤ –ª–æ–≥–∏ –∏–¥—É—Ç —Ç–æ–ª—å–∫–æ –∫–æ–¥—ã, –±–µ–∑ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏
        # '–§–æ—Ç–æ': —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–µ–∑ —Ñ–æ—Ç–æ
    }
    client_name = task.client.name
    client_slug = task.slug

    # –ü–µ—Ä–µ–¥–µ–ª—ã–≤–∞—é –ª–æ–≥–∏ –≤ —Å–ª–æ–≤–∞—Ä—å
    lines = logs.split('\n')[:-2]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 —É–±–∏—Ä–∞—é —Ç.–∫. —Ç–∞–º –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
    logs_dict = {}
    for line in lines:
        try:
            key = line.split('"')[1]
        except IndexError:
            continue
        start = line.index(':') + 1
        end = line.index(';')
        value = []
        for v in line[start:end].split(','):
            v = v.strip()
            if v.isnumeric():
                v = int(v)
            value.append(v)

        if key in ['–û–ø—Ü–∏–∏', '–§–æ—Ç–æ']:  # –û–ø—Ü–∏–∏ –±–µ–∑ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏, –§–æ—Ç–æ —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            logs_dict[key] = pd.Series(value, name=key)
        else:  # –û—Å—Ç–∞–ª—å–Ω—ã–µ —ç—Ç–æ pandas dataframe –≤ –≤–∏–¥–µ –ö–æ–¥–∞ –∏ –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏
            df2 = pd.Series(value, name='–ö–æ–¥', dtype='string')
            joined = pd.merge(template, df2, left_on=lookup_cols[key][0], right_on='–ö–æ–¥')
            joined.drop_duplicates(subset=[lookup_cols[key][0]], inplace=True)
            joined = joined[[lookup_cols[key][0], lookup_cols[key][1]]]
            logs_dict[key] = joined

    file_date = str(datetime.datetime.now()).replace(' ', '_').replace(':', '-')
    logs_save_path = f'converter/{client_slug}/logs/log_{client_slug}_{file_date}.xlsx'
    os.makedirs(os.path.dirname(logs_save_path), exist_ok=True)

    # –ì–æ—Ç–æ–≤—ã–µ –ª–æ–≥–∏ –≤ xlsx
    with pd.ExcelWriter(logs_save_path) as writer:
        for key, value in logs_dict.items():
            df = pd.DataFrame(value)
            # –¢–∞–∫–æ–π –¥–ª–∏–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–æ—Ç–æ—Ä–æ–µ pandas –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            df.T.reset_index().T.to_excel(writer, sheet_name=key, header=False, index=False)

    # –û—Ç–ø—Ä–∞–≤–ª—è—é –ª–æ–≥–∏ –Ω–∞ –ø–æ—á—Ç—É
    if task.notifications_email:
        if any(code in logs_dict for code in list(lookup_cols.keys())) or logs_dict['–§–æ—Ç–æ'].iloc[0] > 0:
            send_email(
                subject='–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä. –î–æ–±–∞–≤–∏—Ç—å –∫–æ–¥—ã',
                body=f'–ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–¥—ã –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ {client_name}.\n–õ–æ–≥–∏:\n{logs}',
                recipients=task.notifications_email,
                attachments=[logs_save_path],
            )

    return logs_save_path


def bot_messages(logs, logs_xlsx, price, task):
    """
    –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞
    :param logs: –ª–æ–≥–∏ –æ—Ç converter_logs
    :param logs_xlsx: –ª–æ–≥–∏ –≤ xlsx –æ—Ç logs_to_xlsx
    :param price: –ø—Ä–∞–π—Å –æ—Ç converter_process_result
    :param task: —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ó–∞–¥–∞—á–∏ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞
    """
    client_slug = task.slug

    # –ü—Ä–∞–π—Å –≤ csv
    file_date = str(datetime.datetime.now()).replace(' ', '_').replace(':', '-')
    price_save_path = f'converter/{client_slug}/prices/price_{client_slug}_{file_date}.csv'

    # string_buffer = io.StringIO()
    # price.to_csv(string_buffer, sep=';', header=True, index=False, decimal=',')
    # csv_string = string_buffer.getvalue()
    #
    # with open(price_save_path, 'w', encoding='cp1251', errors='ignore') as f:
    #     f.write(csv_string)
    price.to_csv(price_save_path, sep=';', header=True, index=False, decimal=',', encoding='cp1251', errors='ignore',
                 lineterminator='\n')

    # –û—Ç–ø—Ä–∞–≤–∫–∞ –ª–æ–≥–æ–≤ –∏ –ø—Ä–∞–π—Å–∞ —á–µ—Ä–µ–∑ –±–æ—Ç–∞ —Ç–µ–ª–µ–≥—Ä–∞–º–∞
    chat_ids = ConverterLogsBotData.objects.all()
    logs = f'üü¢ {task.name}\n\n{logs}'
    for chat_id in chat_ids:
        split_message = break_message_to_parts(logs)
        for message in split_message:
            bot.send_message(chat_id.chat_id, message)
        # if len(logs) > 4095:  # –£ —Ç–µ–ª–µ–≥—Ä–∞–º–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ 4096 —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏
        #     for x in range(0, len(logs), 4095):
        #         bot.send_message(chat_id.chat_id, logs[x:x + 4095])
        # else:
        #     bot.send_message(chat_id.chat_id, logs)
        if logs_xlsx:
            bot.send_document(chat_id.chat_id, InputFile(logs_xlsx))
        bot.send_document(chat_id.chat_id, InputFile(price_save_path))

    os.remove(price_save_path)
    return


def change_vin_to_random(price):
    def modify_vin(vin):
        if len(vin) == 17:
            last_six = vin[-6:]
            if last_six.isdigit():
                # All last 6 characters are digits
                random_number = str(random.randint(0, 999999)).zfill(6)
                return vin[:-6] + random_number
            else:
                # Last 6 characters contain letters
                for i in range(5, -1, -1):
                    if last_six[i].isalpha():
                        # Found the last letter
                        prefix = last_six[:i + 1]
                        digits_part = last_six[i + 1:]
                        if digits_part.isdigit():
                            random_number = str(random.randint(0, int(digits_part))).zfill(len(digits_part))
                            return vin[:-6] + prefix + random_number
                        break
        return vin

    price['VIN'] = price['–ò—Å—Ö–æ–¥–Ω—ã–π VIN'].apply(modify_vin)
    return price


def save_on_ftp(save_path, file):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª –Ω–∞ ftp
    :param save_path: –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    :param file: binary —Ñ–∞–π–ª
    """
    file_path = Path(save_path)
    # with FTP('ph.onllline.ru', env('FTP_LOGIN'), env('FTP_PASSWORD')) as ftp, open(save_path, 'rb') as file:
    with FTP('ph.onllline.ru', env('FTP_LOGIN'), env('FTP_PASSWORD')) as ftp:
        # TODO –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤, –Ω–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω
        cd_tree(ftp, str(file_path.parents[0]))
        file = io.BytesIO(file)
        ftp.storbinary(f'STOR {file_path.name}', file)
        ftp.cwd('/')
    return


def cd_tree(ftp, path):
    """
    –°–æ–∑–¥–∞—ë—Ç –ø–∞–ø–∫–∏ –Ω–∞ ftp –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    :param ftp: FTP –∫–ª–∞—Å—Å –∏–∑ ftplib
    :param path: –ø—É—Ç—å –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–µ–Ω –Ω–∞ ftp
    """
    path = path.replace('\\', '/')  # –ö–æ—Å—Ç—ã–ª—å –¥–ª—è windows
    for folder in path.split('/'):
        try:
            ftp.cwd(folder)
        except ftplib.error_perm:
            ftp.mkd(folder)
            ftp.cwd(folder)
    return


def get_photo_folders():
    """ –ü–æ–ª—É—á–∞–µ—Ç –æ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ —Å–ø–∏—Å–æ–∫ –ø–∞–ø–æ–∫ —Å —Ñ–æ—Ç–æ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –≤ –±–∞–∑—É """
    url = 'http://151.248.118.19/Api/Stock/GetClients'
    response = requests.post(url).json()
    current_folders = [f.folder for f in PhotoFolder.objects.all()]
    new_folders = []
    for folder in response:
        if folder not in current_folders:
            new_folders.append(PhotoFolder(folder=folder))
    PhotoFolder.objects.bulk_create(new_folders)
    return


def get_configurations():
    """ –ü–æ–ª—É—á–∞–µ—Ç –æ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç –≤ –±–∞–∑–µ """
    url = 'http://151.248.118.19/Api/Configurations/GetList'
    response = requests.post(url).json()
    current_configurations = Configuration.objects.all()
    new_configurations = []
    updated_configurations = []
    for conf in response:
        exists = current_configurations.filter(converter_id=conf['id'])
        if exists.count() > 0:
            updated = exists[0]
            updated.name = conf['name']
            updated.configuration = conf['configuration']
            updated_configurations.append(updated)
        else:
            new_configurations.append(Configuration(converter_id=conf['id'], name=conf['name'],
                                                    configuration=conf['configuration']))
    Configuration.objects.bulk_update(updated_configurations, ['name', 'configuration'])
    if len(new_configurations):
        Configuration.objects.bulk_create(new_configurations)
    return


def avilon_custom_task():
    """
    –û—Å–æ–±–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ê–≤–∏–ª–æ–Ω –ü—Ä–µ–º–∏—É–º –í–æ–ª–≥–æ–≥—Ä–∞–¥–∫–∞. –° –ø—Ä–∞–π—Å–∞ –ê–≤–∏–ª–æ–Ω Seres Aito –±–µ—Ä—ë—Ç –ø–æ –æ–¥–Ω–æ–º—É, —Å–∞–º–æ–º—É –¥–µ—à—ë–≤–æ–º—É
    –∞–≤—Ç–æ–º–æ–±–∏–ª—é, –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º. –û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–π–ª –Ω–∞ ftp, –∫–æ—Ç–æ—Ä—ã–π —É –ê–≤–∏–ª–æ–Ω –ü—Ä–µ–º–∏—É–º –í–æ–ª–≥–æ–≥—Ä–∞–¥–∫–∞ –ø—Ä–æ–ø–∏—Å–∞–Ω
    –≤ –ø–æ–ª–µ –î–æ–±–∞–≤–∏—Ç—å –∫ –ø—Ä–∞–π—Å—É.
    :return:
    """
    avilon_seres_aito_task = ConverterTask.objects.get(pk=51)
    avilon_premium_volgogradka_task = ConverterTask.objects.get(pk=22)

    source_file = f'http://ph.onllline.ru/{avilon_seres_aito_task.price}'
    file_to_update = avilon_premium_volgogradka_task.add_to_price.replace('http://ph.onllline.ru/', '')
    temp_file = 'temp/avilon_custom_task.xlsx'

    # –û–±–Ω–æ–≤–ª—è—é –ê–≤–∏–ª–æ–Ω SERES AITO
    get_price(task=avilon_seres_aito_task)

    # –°–æ—Ä—Ç–∏—Ä—É—é –ø–æ –ú–∞—Ä–∫–µ, –ú–æ–¥–µ–ª–∏ –∏ –¶–µ–Ω–µ
    df = pd.read_csv(source_file, decimal=',', sep=';', header=0, encoding='cp1251')
    df = df.sort_values(by=['–ú–∞—Ä–∫–∞', '–ú–æ–¥–µ–ª—å', '–¶–µ–Ω–∞'])

    # –§–∏–ª—å—Ç—Ä—É—é –∏ –±–µ—Ä—É —Å–∞–º—ã–π –¥–µ—à—ë–≤—ã–π –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∏–ª—å—Ç—Ä—É
    filters = [
        {'–ú–∞—Ä–∫–∞': 'AITO', '–ú–æ–¥–µ–ª—å': 'M5'},
        {'–ú–∞—Ä–∫–∞': 'Seres', '–ú–æ–¥–µ–ª—å': 'Aito M7'}
    ]
    result_df = pd.DataFrame()
    for filter_cond in filters:
        filtered_df = df
        for key, value in filter_cond.items():
            filtered_df = filtered_df[filtered_df[key] == value]
        if not filtered_df.empty:
            result_df = pd.concat([result_df, filtered_df.iloc[[0]]], ignore_index=True)

    # –°–æ—Ö—Ä–∞–Ω—è—é –≤ xlsx, –æ—Ç–ø—Ä–∞–≤–ª—è—é –Ω–∞ ftp
    result_df.T.reset_index().T.to_excel(temp_file, sheet_name='data', header=False, index=False)
    with open(temp_file, 'rb') as file:
        file_content = file.read()
        save_on_ftp(file_to_update, file_content)

